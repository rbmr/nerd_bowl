# Course Information

### Study Goals

This course covers all the basic concepts of Statistical Learning, focusing on the classical techniques of Machine Learning before the era of Deep Learning. These concepts include classification, (ridge) regression, (hierarchical) clustering, feature reduction and extraction, model selection and bootstrapping, fairness in ML. The emphasis is on the concepts rather than the mathematical details.

At the end of this course, the student is able to:

1. Explain the fundamental concepts and assumptions in the field of Statistical Learning, like the classical supervised learning, but also regression, clustering and dimensionality reduction
2. Reflect on the strengths and limitations of classical and modern Statistical Learning techniques.
3. Apply the covered learning techniques to analyse a dataset and reason about their effectiveness

### Assessment

The final grade of the course consists of the following components:

- Digital Exam: weblab exam (weighting 70%)
- Group Report: project report on analysis of a dataset (weighting 30%)

Final grade calculation = 0.7 * Digital Exam + 0.3 * Group Report

A passing final grade for the course can only be earned when for all components at least a 5.0 is earned, and the weighted final grade is at least a 5.8.

In case of an insufficient final result, repair options may exist in accordance with Article 17A, Times and number of examinations, sub 1, of the Teaching and Examination Regulations, for:

- Digital Exam: Resit opportunity
- Group Report: Repair opportunity

Disclaimer: information may change depending on unforeseen circumstances or measures (see: TER Art 2, sub 5).

# Summary

## Basic Machine Learning: classification
**General Formulation of ML**
Given a model family $f(\textbf{x};\theta)$ with parameters $\theta$, a loss function $l(\hat y ,y)$, and a regularizer $\Omega(\theta)$, for feature vector $\textbf{x}$, predicted value $\hat y$, and true value $y$, find: $$\arg \min_\theta L(\theta) \text{ , with } L(\theta)=\sum_{i=1}^N l(f(\textbf{x}_i;\theta),y_i)+\Omega(\theta)$$
### Classification Evaluation
#### Asymmetric Error Metrics

| **Metric**                                   | **Calculation**                 | **Description**                                                                         |
| -------------------------------------------- | ------------------------------- | --------------------------------------------------------------------------------------- |
| Sensitivity <br>(Recall, True Positive Rate) | $\frac {TP} P=\frac{TP}{TP+FN}$ | *Of all the _real_ positive cases out there, what fraction did my model find?*          |
| Specificity<br>(True Negative Rate)          | $\frac{TN}{N}=\frac{TN}{TN+FP}$ | *The percentage/rate of the 'negative' class that were correctly identified*            |
| Precision                                    | $\frac{TP}{TP+FP}$              | *Of all the real negative cases out there, what fraction did my model correctly clear?* |
**ROC & AUC**
- **ROC (Receiver-Operator Characteristic) Curve** This curve is generated by plotting the *Sensitivity* against the *False Positive Rate (1-Specificity)*, while varying the classifier's decision threshold. 
	- You make the curve by sliding this decision threshold from 1.0 to 0.0, where 1.0 means no positive are given and 0.0 only positives are given. 
- **AUC (Area Under the ROC Curve)** This single number summary of the ROC curve is a standard performance measure. It is not sensitive to class imbalance in the test set. A random classifier has an AUC of 0.5

**Precision-Recall (P-R) Curves**
- This curve plots precision against recall (sensitivity).
- **PRAUC** This is the summary metric for the P-R Curve
	- A perfect classifier has a PRAUC of 1.
	- Unlike AUC, the baseline for a random classifier is not 0.5; it is prior probability of the positive class. 

### Multi-class Classification
**Voting Schemes**
To convert binary classifiers into multi-class classifiers, we can use voting schemes.
- One-vs-Rest (OvR):
	- For K classes, train K binary classifiers that predict "Is it class *i* or another"
	- To classify a new object, you run it through all K classifiers.
	- Each classifier gives a score. The new object is assigned to the class whose classifier gives the highest score.
- One-vs-One (OvO):
	- You train a classifier for all possible combinations of classes
	- Run each classifier on a data point.
	- The class that gets the most votes wins.


**Platt Scaling**
The "voting" methods have a flaw. Classifiers like SVMs don't output a nice, clean probability between 0 and 1. They output a raw "score" or "distance from the decision boundary". These scores are not directly comparable. A score of 0.8 from the "apple-vs-pear" classifier might mean something totally different than a score of 0.8 from the "apple-vs-banana" classifier.
The solution is to train a simple logistic function to map the classifier's score to a 0-to-1 range. 

**Calibration**
Calibration is the ultimate goal of techniques like Platt scaling. A classifier is calibrated if its output probabilities reflect the true likelihood. 
In a high-stakes field, like medicine or finance, you need to know you can trust the output of a model, and also be confident about it's confidence of it's outcome. 
For example, would you trust a model that says it has a 99% likelihood of raining, even though it is currently sunny outside with no clouds in sight?

### Practical Considerations in ML
- Missing Data
	- Data can be missing at random (MCAR), or missing at random (MAR), or Missing not at random (MNAR)
	- To resolve this: 
		- Remove incomplete rows
		- Impute values based on a best guess
		- Use models that naturally handle missing values (e.g. decision trees)
- Feature Scaling
	- Rescaling features affects classifiers that rely on distance
	- Affected: Nearest Mean, kNN, Parzen, Perceptron, SVMs
	- Not-Affected: QDC, LDC, Neural Networks, and Decision Trees
- Sources of Variation
	- To reduce high variance in the test error, you need to get more test data, use a simpler model, or reduce the number of hyper parameters. 


## Computational Learning Theory
## Regression
## Feature Reduction
## Clustering
## Practical ML and fairness
